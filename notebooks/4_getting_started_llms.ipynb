{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with LLMs and RAG\n",
    "\n",
    "Note: First create a filtered dataset with `filter-dataset.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T11:12:28.784978Z",
     "start_time": "2025-03-21T11:12:28.772943Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import pickle\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.llm import get_azure_embeddings_client, get_llm_client, get_gemini_llm_client\n",
    "\n",
    "if not load_dotenv():\n",
    "    raise Exception('Error loading .env file. Make sure to place valid keys in the .env file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T11:14:35.644042Z",
     "start_time": "2025-03-21T11:14:35.632207Z"
    }
   },
   "outputs": [],
   "source": [
    "ARTICLES_CLEAN_DIR = os.path.join(\"..\", \"data\", \"articles_clean\")\n",
    "FILTERED_METADATA_PATH = os.path.join(\"..\", \"data\", \"filtered_metadata.csv\")\n",
    "DB_PATH = os.path.join(\"..\", \"data\", \"db\", \"sample.db\")\n",
    "\n",
    "if not os.path.exists(DB_PATH):\n",
    "    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T11:15:00.943850Z",
     "start_time": "2025-03-21T11:15:00.903439Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_metadata = pd.read_csv(FILTERED_METADATA_PATH)\n",
    "filtered_metadata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create simple vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_from_path(filenames: list[str]) -> [Document]:\n",
    "    documents = []\n",
    "    \n",
    "    for file_name in filenames:\n",
    "        file_path = os.path.join(ARTICLES_CLEAN_DIR, file_name)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            file = json.load(file)\n",
    "\n",
    "        text = file.get(\"text\", \"\")\n",
    "        documents.append(Document(page_content=text, metadata={\n",
    "            \"title\": file.get(\"title\", \"\"),\n",
    "            \"author\": file.get(\"author\", \"\"),\n",
    "            \"published_at\": file.get(\"published_at\", \"\"),\n",
    "            \"id\": file.get(\"id\", \"\"),\n",
    "        }))\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documents_from_path(filtered_metadata[\"filename\"])\n",
    "print(f\"Number of articles: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, separators=[\"\\n\\n\", \"\\n\"])\n",
    "\n",
    "# Split documents and create vector database\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_azure_embeddings_client(\n",
    "    chunk_size=512, # number of documents' chunks processed in parallel, decrease if you hit rate limits\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count build embedding token number\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "build_token_count = sum([len(tokenizer.encode(doc.page_content)) for doc in texts])\n",
    "print(f\"Token count: {build_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the database\n",
    "with open(DB_PATH, \"wb\") as f:\n",
    "    pickle.dump(db.serialize_to_bytes(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT: Load vector DB\n",
    "with open(DB_PATH, \"rb\") as f:\n",
    "    serialized_data = pickle.load(f)\n",
    "\n",
    "# Reconstruct the FAISS database\n",
    "db = FAISS.deserialize_from_bytes(serialized_data, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI: free tier Gemini LLM \n",
    "# rate_limiter = InMemoryRateLimiter(\n",
    "#     requests_per_second=0.5,  # <-- Gemini Free Tier\n",
    "#     check_every_n_seconds=0.1,\n",
    "# )\n",
    "\n",
    "# llm = get_gemini_llm_client(\n",
    "#     max_tokens=1024,\n",
    "#     temperature=0.2,\n",
    "#     rate_limiter=rate_limiter,\n",
    "# )\n",
    "\n",
    "# Default go-to Openrouter LLM - check README for other available models\n",
    "llm = get_llm_client(\n",
    "    # Configurable parameters\n",
    "    max_tokens=1024,\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert assistant. Use only the following retrieved context to answer the question accurately and concisely. \n",
    "If nothing is mentioned in the context, say \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], \n",
    "    template=system_prompt\n",
    ")\n",
    "\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query):\n",
    "    response = retrieval_chain.invoke({\"query\": query})\n",
    "    print(f\"Question: {query}\\nAnswer: {response['result']}\")\n",
    "    print(\"\\nSources: \\n\")\n",
    "    for source in response[\"source_documents\"]:\n",
    "        print(source.metadata)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_question(\"What are the current economic threats in Austria?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
